---
title: "Introduction to Bayesian regression models"
subtitle: "01 - Basics"
author: "Stefano Coretta"
institute: "University of Edinburgh"
editor: source
format:
  mono-light-revealjs:
    theme: [default, custom.scss]
    history: false
    preview-links: true
filters:
  - tachyonsextra
execute: 
  echo: true
bibliography: references.bib
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
theme_set(theme_minimal())

library(lme4)
library(lmerTest)
library(ggeffects)
library(brms)
library(tidybayes)
library(bayesplot)
```

## Download the workshop materials

::: box-error
-   Download the workshop repository from: <https://github.com/stefanocoretta/learnBayes>.
:::

## Schedule

| Times | Topic |
|-------|-------|
|       |       |
|       |       |
|       |       |
|       |       |
|       |       |
|       |       |
|       |       |
|       |       |

## 

![](/img/One_Ring_inscription.svg.png){fig-align="right" width="300"}

::: {.f2 style="text-align: right;"}
One **model** to rule them all.<br> One **model** to find them.<br> One **model** to bring them all<br> And in the darkness bind them.
:::

. . .

::: f5
(No, I couldn't translate 'model' into Black Speech, alas...)
:::

## Now enter The Regression Model

<p style="text-align:center">

<iframe src="https://giphy.com/embed/l4FGCymGGNTZVZwtO" width="480" height="200" frameBorder="0" class="giphy-embed" allowFullScreen>

</iframe>

</p>

## Now enter The Bayesian Regression Model

<p style="text-align:center">

<iframe src="https://giphy.com/embed/3ov9jG4eqz9k3XXsU8" width="480" height="480" frameBorder="0" class="giphy-embed" allowFullScreen>

</iframe>

</p>

## All about the Bayes

::: box-note
Within the **NHST (Frequentist) framework**, the main analysis output is:

-   **Point estimates** of predictors' parameters (with standard error).
-   ***p*****-values**.
:::

. . .

::: box-warning
See @gigerenzer2004, @gigerenzer2004a, @gigerenzer2018 for problems with the "Null Ritual".
:::

. . .

::: box-tip
Within the **Bayesian framework**, the main analysis output is:

-   **Probability distributions** of predictors' parameters.
:::

## An example: MALD

::: box-tip
[Massive Auditory Lexical Decision](https://aphl.artsrn.ualberta.ca/?page_id=827) (MALD) data [@tucker2019]:

-   **Auditory Lexical Decision task** with real and nonce English words.
-   Reaction times and accuracy.
-   Total 521 subjects; subset of 30 subjects, 100 observations each.
:::

## MALD data

```{r}
#| label: mald
#| echo: false

mald <- readRDS("./data/mald.rds")
```

```{r}
#| label: mald-print

mald
```

## MALD: phone-level distance and lexical status

Let's investigate the effects of the following on Reaction Times (RTs):

-   `PhonLev`: mean phone-level Levenshtein distance of the item from all other entries.
    -   An alternative to neighbourhood density.
-   `IsWord`: lexical status (real word vs nonce word).

## What are your expectations?

<div>

<iframe allowfullscreen frameborder="0" height="600px" mozallowfullscreen style="min-width: 500px; min-height: 355px" src="https://app.wooclap.com/events/JEJPPA/questions/682f25c769100c24662fef2a" width="100%">

</iframe>

</div>

## What are your expectations?

<div>

<iframe allowfullscreen frameborder="0" height="600px" mozallowfullscreen style="min-width: 500px; min-height: 355px" src="https://app.wooclap.com/events/JEJPPA/questions/682f260f776b2a0921ae54ed" width="100%">

</iframe>

</div>

## MALD: phone-level distance and lexical status

```{r}
#| label: fig-dist-stat
#| fig-asp: 0.75
#| echo: false
#| fig-cap: "Scatter plot of mean phon-level distance vs reaction times."

mald |> 
  ggplot(aes(PhonLev, RT)) +
  geom_point(alpha = 0.1) +
  geom_smooth(aes(colour = IsWord, fill = IsWord), method = "lm", formula = y ~ x) +
  labs(
    x = "Phone-level distance", y = "RT (ms)"
  )
```

## A frequentist regression model

```{r}
#| label: lm-1

lm_1 <- lmer(
  RT ~
    PhonLev +
    IsWord +
    PhonLev:IsWord +
    (1 | Subject),
  data = mald
)
```

## A frequentist regression model: summary

```{r}
#| label: lm-1-summ

summary(lm_1)
```

## A frequentist regression model: plot predictions

```{r}
#| label: fig-lm-1-pred
#| fig-asp: 0.75
#| fig-cap: "Predicted Reaction Times based on a frequentist regression model."
#| echo: false
#| message: false

ggpredict(lm_1, terms = c("PhonLev", "IsWord")) |>
  plot()
```

## Increase model complexity

```{r}
#| label: lm-2
#| eval: false

lm_2 <- lmer(
  RT ~
    PhonLev +
    IsWord +
    PhonLev:IsWord +
    (PhonLev*IsWord | Subject),
  data = mald
)
```

. . .

```{r}
#| label: lm-2-run
#| echo: false
#| warning: true

lm_2 <- lmer(
  RT ~
    PhonLev +
    IsWord +
    PhonLev:IsWord +
    (PhonLev*IsWord | Subject),
  data = mald
)
```

<p style="text-align:center">

<iframe src="https://giphy.com/embed/3o7abwbzKeaRksvVaE" width="480" height="204" frameBorder="0" class="giphy-embed" allowFullScreen>

</iframe>

</p>

## Let's go Bayesian! {background-color="var(--inverse)"}

![](/img/the-bayesian-rats.jpeg){fig-align="center" width="500"}

## A Bayesian regression model

```{r}
#| label: brm-1
#| eval: false

library(brms)

brm_1 <- brm(
  RT ~
    PhonLev +
    IsWord +
    PhonLev:IsWord +
    (PhonLev * IsWord | Subject),
  data = mald,
  family = gaussian()
)
```

```{r}
#| label: brm-1-run
#| echo: false

brm_1 <- brm(
  RT ~
    PhonLev +
    IsWord +
    PhonLev:IsWord +
    (PhonLev * IsWord | Subject),
  data = mald,
  family = gaussian(),
  # Technical stuff
  cores = 4,
  iter = 4000,
  file = "data/cache/brm_1"
)
```

## A Bayesian regression model: summary

```{r}
#| label: brm-1-summ
#| warning: false

brm_1
```

## A Bayesian regression model: plot predictions

```{r}
#| label: fig-brm-1-plot
#| fig-cap: "Predicted RTs based on a Bayesian regression model."
#| fig-asp: 0.75
#| echo: false

conditional_effects(brm_1, effects = "PhonLev:IsWord")
```

## Let's start small: A Gaussian model

$$
RT \sim Gaussian(\mu, \sigma)
$$

-   Reaction Times $RT$

-   distributed according to $\sim$

-   a Gaussian distribution $Gaussian()$

-   with mean $\mu$ and standard deviation $\sigma$.

. . .

::: box-warning
We start with a Gaussian model for pedagogical reasons. You would usually start with the model you need to answer your research questions.
:::

## Uncertainty of parameters

$$
\begin{align}
RT & \sim Gaussian(\mu, \sigma)\\
\mu & \sim Student(\mu_1, \sigma_1)\\
\sigma & \sim Student(\mu_2, \sigma_2)
\end{align}
$$

. . .

::: box-note
-   Uncertainty can be expressed with probability distributions, here Student-t distributions (the default in brms).

-   The Gaussian model estimates the probability distributions of $\mu$ and $\sigma$ from the data.

-   These probability distributions are called **posterior probability distributions**.

-   They can be summarised with $\mu_1, \sigma_1, \mu_2, \sigma_2$.
:::

## Gaussian model of RTs

**Try it yourself!**

```{r}
#| label: brm-2
#| eval: false

brm_2 <- brm(
  # formula = 
  RT ~ 1,
  data = mald,
  # Specify the distribution family of the outcome
  family = gaussian()
)
```

. . .

::: box-tip
-   The model estimates the probability distributions of $\mu$ and $\sigma$.

-   To do so, a **sampling algorithm** is used (Markov Chain Monte Carlo, **MCMC**).
:::

## MCMC what? {background-color="var(--inverse)"}

**Markov Chain Monte Carlo**

-   MCMC simulation: <https://chi-feng.github.io/mcmc-demo/app.html>

-   More on MCMC

    -   <https://elizabethpankratz.github.io/bayes_stat/day1/mcmc.html>

    -   <http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/>

## Technicalities

```{r}
#| label: brm-2-explain

brm_2 <- brm(
  RT ~ 1,
  data = mald,
  family = gaussian(),
  
  # TECHNICAL STUFF
  # Save model output to file
  file = "./data/cache/brm_2.rds",
  # Number of MCMC chains
  chains = 4, 
  # Number of iterations per chain
  iter = 2000,
  # Number of cores to use (one per chain)
  cores = 4
)
```

::: box-error
You can find out how many cores your laptop has with `parallel::detectCores()`.
:::

## The model summary

```{r}
#| label: brm-2-summ

brm_2
```

## Interpreting the summary: $\mu$

```{r}
#| label: brm-2-summ-2
#| echo: false

cat(capture.output(summary(brm_2))[8:11], sep = "\n")
```

::: box-warning
`Intercept` is $\mu$.

$$
\begin{align}
RT & \sim Gaussian(\mu, \sigma)\\
\mu & \sim Student(1046.57, 6.38)\\
\end{align}
$$

-   The posterior probability distribution of $\mu$, i.e. of the mean RTs, is $Student(1047, 6)$.

-   There is a **95% probability** that the mean RTs is **between 1034 and 1059 ms**, conditional on the model and data. This is the 95% Credible Interval (CrI) of $Student(1047, 6)$.
:::

## Interpret the summary: $\sigma$

```{r}
#| label: brm-2-summ-3
#| echo: false

cat(capture.output(summary(brm_2))[12:14], sep = "\n")
```

::: box-warning
`sigma` is $\sigma$.

$$
\begin{align}
RT & \sim Gaussian(\mu, \sigma)\\
\mu & \sim Student(1046.57, 6.38)\\
\sigma & \sim Student(347.57, 4.52)
\end{align}
$$

-   The posterior probability distribution of $\sigma$, i.e. of the standard deviation of RTs, is $Student(348, 5)$.

-   There is a **95% probability** that the RT standard deviation is **between 339 and 357 ms**, conditional on the model and data.
:::

## Posterior probabilities

::: box-note
-   These are **posterior probability distributions**.
-   They are always conditional on model and data.
-   The summary reports **95% Credible Intervals (CrIs)**, but there is nothing special about 95% and you should obtain a set of intervals.
:::

## Model plot

```{r}
#| label: fig-brm-2-plot
#| fig-cap: "Density and trace plot of the Bayesian Gaussian model `brm_2`."
#| fig-asp: 0.75

plot(brm_2, combo = c("dens", "trace"))

```

## Extract the MCMC draws

```{r}
#| label: brm-2-draws

brm_2_draws <- as_draws_df(brm_2)
brm_2_draws
```

## Get parameters

To list all the names of the parameters in a model, use:

```{r}
#| label: brm-2-vars

variables(brm_2)
```

## Posterior distributions: intercept

Now you can plot the draws using ggplot2.

```{r}
#| label: brm-2-int
#| output-location: slide

brm_2_draws |>
  ggplot(aes(b_Intercept)) +
  stat_halfeye(fill = "#214d65", alpha = 0.8) +
  scale_x_continuous() +
  labs(title = "Posterior distribution of Intercept")
```

## Posterior distribution: `IsWordFalse`

```{r}
#| label: brm-2-isword
#| output-location: slide
#| eval: false

brm_2_draws |>
  ggplot(aes(b_IsWordFALSE)) +
  stat_halfeye(fill = "#624B27", alpha = 0.8) +
  scale_x_continuous() +
  labs(title = "Posterior distribution of IsWord: FALSE - TRUE")
```

## Conditional posterior probabilities

```{r}
#| label: brm-2-cond
#| eval: false

conditional_effects(brm_2, "IsWord")
```

## Summary

::: box-note
-   Bayesian statistics is a **framework for doing inference** (i.e. learning something about a population from a sample).

-   The Bayesian framework is based on **estimating uncertainty with probability distributions** on parameters.

-   The **syntax** you know from `[g]lm[er]()` works with `brm()` from the brms package.

-   Results allow you to make statements like "There is a 95% probability that the difference between A and B is between *q* and *w*".
:::

## References
