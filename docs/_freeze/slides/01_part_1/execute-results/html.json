{
  "hash": "e7343908bced1ed762296e7dfe111241",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Bayesian Linear Models\"\nsubtitle: \"Part 1\"\nauthor: \"Stefano Coretta\"\ninstitute: \"University of Edinburgh\"\neditor: source\nformat:\n  mono-light-revealjs:\n    theme: [default, custom.scss]\n    history: false\nfilters:\n  - tachyonsextra\nexecute: \n  echo: true\n---\n\n\n\n\n## \n\n![](/img/One_Ring_inscription.svg.png){fig-align=\"right\" width=\"300\"}\n\n::: {.f2 style=\"text-align: right;\"}\nOne **model** to rule them all.<br> One **model** to find them.<br> One **model** to bring them all<br> And in the darkness bind them.\n:::\n\n. . .\n\n::: f5\n(No, I couldn't translate 'model' into Black Speech, alas...)\n:::\n\n## Now enter The Linear Model\n\n<p style=\"text-align:center\">\n\n<iframe src=\"https://giphy.com/embed/l4FGCymGGNTZVZwtO\" width=\"480\" height=\"200\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen>\n\n</iframe>\n\n</p>\n\n## Now enter The Bayesian Linear Model\n\n<p style=\"text-align:center\">\n\n<iframe src=\"https://giphy.com/embed/3ov9jG4eqz9k3XXsU8\" width=\"480\" height=\"480\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen>\n\n</iframe>\n\n</p>\n\n## All about the Bayes\n\n::: box-note\nWithin the **NHST (Frequentist) framework**, the main analysis output is:\n\n-   **Point estimates** of predictors' parameters (with standard error).\n-   **P-values**. ðŸ˜ˆ\n:::\n\n. . .\n\n::: box-tip\nWithin the **Bayesian framework**, the main analysis output is:\n\n-   **Probability distributions** of predictors' parameters.\n:::\n\n## An example: MALD\n\n::: box-tip\n[Massive Auditory Lexical Decision](https://aphl.artsrn.ualberta.ca/?page_id=827) (MALD) data (Tucker et al. 2019):\n\n-   **Auditory Lexical Decision task** with real and nonce English words.\n-   Reaction times and accuracy.\n-   Total 521 subjects; subset of 30 subjects, 100 observations each.\n:::\n\n. . .\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmald\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3,000 Ã— 7\n   Subject Item         IsWord PhonLev    RT ACC       RT_log\n   <chr>   <chr>        <fct>    <dbl> <int> <fct>      <dbl>\n 1 15345   nihnaxr      FALSE     5.84   945 correct     6.85\n 2 15345   skaep        FALSE     6.03  1046 incorrect   6.95\n 3 15345   grandparents TRUE     10.3    797 correct     6.68\n 4 15345   sehs         FALSE     5.88  2134 correct     7.67\n 5 15345   cousin       TRUE      5.78   597 correct     6.39\n 6 15345   blowup       TRUE      6.03   716 correct     6.57\n 7 15345   hhehrnzmaxn  FALSE     7.30  1985 correct     7.59\n 8 15345   mantic       TRUE      6.21  1591 correct     7.37\n 9 15345   notable      TRUE      6.82   620 correct     6.43\n10 15345   prowthihviht FALSE     7.68  1205 correct     7.09\n# â„¹ 2,990 more rows\n```\n\n\n:::\n:::\n\n\n## MALD: phone-level distance and lexical status\n\nLet's investigate the effects of:\n\n-   `PhonLev`: mean phone-level Levenshtein distance.\n-   `IsWord`: lexical status (real word vs nonce word).\n\nOn:\n\n-   `RT`: reaction times.\n-   `ACC`: accuracy.\n\n## MALD: phone-level distance and lexical status\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmald |> \n  ggplot(aes(PhonLev, RT)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(aes(colour = IsWord, fill = IsWord), method = \"lm\", formula = y ~ x) +\n  labs(\n    x = \"Phone-level distance\", y = \"RT (ms)\"\n  )\n```\n\n::: {.cell-output-display}\n![](01_part_1_files/figure-revealjs/dist-stat-1.png){width=960}\n:::\n:::\n\n\n## A frequentist linear model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_1 <- lmer(\n  RT ~\n    PhonLev +\n    IsWord +\n    PhonLev:IsWord +\n    (1 | Subject),\n  data = mald\n)\n```\n:::\n\n\n## A frequentist linear model: summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ PhonLev + IsWord + PhonLev:IsWord + (1 | Subject)\n   Data: mald\n\nREML criterion at convergence: 43232.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5511 -0.5985 -0.2439  0.3185  5.6906 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept)  11032   105.0   \n Residual             104648   323.5   \nNumber of obs: 3000, groups:  Subject, 30\n\nFixed effects:\n                    Estimate Std. Error t value\n(Intercept)          754.965     49.687  15.195\nPhonLev               32.148      6.389   5.032\nIsWordFALSE          212.440     65.453   3.246\nPhonLev:IsWordFALSE  -11.620      9.076  -1.280\n\nCorrelation of Fixed Effects:\n            (Intr) PhonLv IWFALS\nPhonLev     -0.907              \nIsWordFALSE -0.647  0.690       \nPhL:IWFALSE  0.640 -0.705 -0.983\n```\n\n\n:::\n:::\n\n\n## A frequentist linear model: plot predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggpredict(lm_1, terms = c(\"PhonLev\", \"IsWord\")) %>%\n  plot()\n```\n\n::: {.cell-output-display}\n![](01_part_1_files/figure-revealjs/lm-1-pred-1.png){width=960}\n:::\n:::\n\n\n## Increase model complexity\n\n**Try it yourself!** Does it work?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_2 <- lmer(\n  RT ~\n    PhonLev +\n    IsWord +\n    PhonLev:IsWord +\n    (PhonLev + IsWord | Subject),\n  data = mald\n)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0430116 (tol = 0.002, component 1)\n```\n\n\n:::\n:::\n\n\n<p style=\"text-align:center\">\n\n<iframe src=\"https://giphy.com/embed/3o7abwbzKeaRksvVaE\" width=\"480\" height=\"204\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen>\n\n</iframe>\n\n</p>\n\n## Let's go Bayesian! {background-color=\"var(--inverse)\"}\n\n...like the Bayesian Rats!\n\n![](/img/the-bayesian-rats.jpeg){fig-align=\"center\" width=\"500\"}\n\n## A Bayesian linear model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_1 <- brm(\n  RT ~\n    PhonLev +\n    IsWord +\n    PhonLev:IsWord +\n    (PhonLev + IsWord | Subject),\n  data = mald,\n  family = gaussian()\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n## A Bayesian linear model: summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ PhonLev + IsWord + PhonLev:IsWord + (PhonLev + IsWord | Subject) \n   Data: mald (Number of observations: 3000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~Subject (Number of levels: 30) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                 98.77     33.49    44.25   177.00 1.01     1156\nsd(PhonLev)                    7.76      5.03     0.47    19.03 1.01      465\nsd(IsWordFALSE)               97.43     19.11    62.96   139.07 1.00     1804\ncor(Intercept,PhonLev)        -0.42      0.44    -0.93     0.68 1.00     1522\ncor(Intercept,IsWordFALSE)     0.53      0.27    -0.05     0.94 1.01      589\ncor(PhonLev,IsWordFALSE)      -0.36      0.44    -0.94     0.65 1.02      384\n                           Tail_ESS\nsd(Intercept)                  1261\nsd(PhonLev)                    1031\nsd(IsWordFALSE)                2778\ncor(Intercept,PhonLev)         2153\ncor(Intercept,IsWordFALSE)     1515\ncor(PhonLev,IsWordFALSE)       1057\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             755.18     49.73   659.14   853.47 1.00     2234     1457\nPhonLev                31.94      6.60    18.84    44.46 1.00     2510     2759\nIsWordFALSE           208.11     67.09    75.56   339.99 1.00     2523     2380\nPhonLev:IsWordFALSE   -11.10      9.04   -28.42     6.65 1.00     2493     2788\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   320.27      4.25   312.03   328.73 1.00     5408     2783\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## A Bayesian linear model: plot predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(brm_1, effects = \"PhonLev:IsWord\")\n```\n\n::: {.cell-output-display}\n![](01_part_1_files/figure-revealjs/brm-1-plot-1.png){width=960}\n:::\n:::\n\n\n## Let's start small\n\n**Try it yourself!**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_2 <- brm(\n  # formula = outcome ~ predictor\n  RT ~ IsWord,\n  data = mald,\n  # Specify the distribution family of the outcome\n  family = gaussian()\n)\n```\n:::\n\n\n. . .\n\n::: box-tip\n-   The model estimates the probability distributions of the effects of each predictor.\n\n-   To do so, a **sampling algorithm** is used (Markov Chain Monte Carlo, **MCMC**).\n:::\n\n## MCMC what? {background-color=\"var(--inverse)\"}\n\n**Markov Chain Monte Carlo**\n\n-   MCMC simulation: <https://chi-feng.github.io/mcmc-demo/app.html>\n\n-   More on MCMC: <http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/>\n\n## Technicalities\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_2 <- brm(\n  RT ~ IsWord,\n  data = mald,\n  family = gaussian(),\n  \n  # TECHNICAL STUFF\n  # Save model output to file\n  file = \"./data/cache/brm_2.rds\",\n  # Number of MCMC chains\n  chains = 4, \n  # Number of iterations per chain\n  iter = 2000,\n  # Number of cores to use (one per chain)\n  cores = 4\n)\n```\n:::\n\n\n::: box-error\nYou can find out how many cores your laptop has with `parallel::detectCores()`.\n:::\n\n## The model summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ IsWord \n   Data: mald (Number of observations: 3000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     981.35      8.73   964.38   998.37 1.00     3970     3032\nIsWordFALSE   132.87     12.63   107.91   156.87 1.00     3936     2937\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   341.21      4.48   332.33   349.82 1.00     4223     3006\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Interpreting the summary\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     981.35      8.73   964.38   998.37 1.00     3970     3032\nIsWordFALSE   132.87     12.63   107.91   156.87 1.00     3936     2937\n```\n\n\n:::\n:::\n\n\n::: box-warning\n`Intercept`: There is a **95% probability** that (based on model and data) the mean RT when the word is real is **between 964 and 999 ms**.\n\n-   The probability distribution of the intercept has mean = 981 ms and SD = 9 (rounded).\n:::\n\n. . .\n\n::: box-tip\n`IsWordFALSE`: At **95% confidence**, the difference in RT between non-words and real words is **between +109 and +158 ms** (based on model and data).\n\n-   The probability distribution of `IsWordFALSE` has mean = 133 ms and SD = 13 (rounded).\n:::\n\n. . .\n\n## Posterior probabilities\n\n::: box-note\n-   These are **posterior probability distributions**.\n-   They are always conditional on model and data.\n-   The summary reports **95% Credible Intervals**, but you can get other intervals too (there is nothing special about 95%).\n:::\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(brm_2, prob = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ IsWord \n   Data: mald (Number of observations: 3000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept     981.35      8.73   970.04   992.96 1.00     3970     3032\nIsWordFALSE   132.87     12.63   116.58   148.84 1.00     3936     2937\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma   341.21      4.48   335.59   346.97 1.00     4223     3006\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Quick plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(brm_2, combo = c(\"dens\", \"trace\"))\n```\n\n::: {.cell-output-display}\n![](01_part_1_files/figure-revealjs/brm-2-plot-1.png){width=960}\n:::\n:::\n\n\n## Extract the MCMC draws\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_2_draws <- as_draws_df(brm_2)\nbrm_2_draws\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A draws_df: 1000 iterations, 4 chains, and 6 variables\n   b_Intercept b_IsWordFALSE sigma Intercept lprior   lp__\n1          981           144   349      1052    -13 -21763\n2          989           131   343      1053    -13 -21761\n3          973           138   338      1040    -13 -21761\n4          983           126   337      1045    -13 -21761\n5          996           114   346      1052    -13 -21762\n6          966           156   339      1042    -13 -21762\n7          973           140   343      1042    -13 -21761\n8          996           127   344      1058    -13 -21762\n9          990           123   340      1050    -13 -21761\n10         970           145   341      1042    -13 -21761\n# ... with 3990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n```\n\n\n:::\n:::\n\n\n## Get parameters\n\nTo list all the names of the parameters in a model, use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvariables(brm_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"b_Intercept\"   \"b_IsWordFALSE\" \"sigma\"         \"Intercept\"    \n[5] \"lprior\"        \"lp__\"         \n```\n\n\n:::\n:::\n\n\n## Posterior distributions\n\nNow you can plot the draws using ggplot2.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nbrm_2_draws %>%\n  ggplot(aes(b_Intercept)) +\n  stat_halfeye(fill = \"#214d65\", alpha = 0.8) +\n  scale_x_continuous() +\n  labs(title = \"Posterior distribution of Intercept\")\n```\n\n::: {.cell-output-display}\n![](01_part_1_files/figure-revealjs/brm-2-int-g-1.png){width=960}\n:::\n:::\n",
    "supporting": [
      "01_part_1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}